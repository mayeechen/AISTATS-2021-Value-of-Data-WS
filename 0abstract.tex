Labeling data for modern machine learning is expensive and time-consuming. Latent variable models can be used to infer labels from weaker, easier-to-acquire sources operating on unlabeled data.
Such models can also be trained using labeled data, presenting a key question: should a user invest in few labeled or many unlabeled points? We answer this via a framework centered on model misspecification in method-of-moments latent variable estimation.
%
Our core result is an exact bias-variance decomposition of the generalization error, which shows that the unlabeled-only approach incurs additional bias under misspecification. We introduce a correction that provably removes this bias in certain cases.
%
We apply our decomposition to three scenarios---well-specified, misspecified, and corrected models---to 1) choose between labeled and unlabeled data and 2) learn from their combination. We observe theoretically and with synthetic experiments that for well-specified models, labeled points are worth a constant factor more than unlabeled points. With misspecification, their relative value is higher due to the additional bias but is reduced with correction. We also apply our approach to study real-world weak supervision techniques for dataset construction.


%OLD
%Labeling data for modern machine learning is expensive and time-consuming while unlabeled data is is generally less informative but easier to acquire, presenting a tradeoff. %\steve{I think this first sentence could be improved to make the tradeoff make more sense: ``Labeling data for modern machine learning is expensive and time-consuming while unlabeled data is generally less informative but easier to acquire, presenting a tradeoff''} %This presents a key tradeoff: should a user invest in few labeled or many unlabeled points? 
%We consider probabilistic models for generating labels, for which both labeled and unlabeled data can be used as input. In the former case, these models have directly computable parameters, while in the latter, they reduce to \textit{latent variable models} where approaches such as method-of-moments are needed to infer labels. 
%Motivated by weak supervision as an application, we study the impact of model misspecification on method-of-moments estimators and their ability to generate accurate labels. Our core result is an exact bias-variance decomposition of the generalization error for such models, which shows that the unlabeled-only approach incurs additional bias under misspecification. We introduce a corrected estimator that provably removes this bias under certain conditions.
%We apply our decomposition framework to three scenarios---well-specified models, misspecified models, and misspecified models with our corrected estimator---to 1) assess labeled vs unlabeled data and 2) learn from a combination of both. We observe theoretically and with synthetic experiments that for well-specified models, labeled points are worth a constant factor more than unlabeled points in terms of generalization error. For misspecified models, their relative value based on our framework is higher due to the additional bias in the unlabeled approach but can be reduced when our correction mechanism is applied.


% OLD OLD
%Labeling data for modern machine learning is expensive and time-consuming. Weak supervision (WS) combines unlabeled data and weak sources to learn a model that generates a large labeled dataset. Such models can also benefit from labeled data, presenting a key tradeoff: should a user invest in few labeled or many unlabeled points? 
%We present a framework centered on model misspecification in WS algorithms. Our core result is an exact bias-variance decomposition of the generalization error, which shows that the unlabeled-only approach incurs additional bias under misspecification. We introduce a correction mechanism that is broadly applicable to method-of-moments approaches and provably removes this bias under certain conditions.
%We apply our decomposition framework to three scenarios---well-specified models, misspecified models, and misspecified models with our correction mechanism---to 1) choose between using labeled vs unlabeled data and 2) learn from a combination of both. We observe theoretically and with synthetic experiments that for well-specified models, labeled points are worth a constant factor more than unlabeled points in terms of generalization error. For misspecified models, their relative value based on our framework is higher due to the additional bias in the unlabeled approach but can be reduced when our correction mechanism is applied.

%the relative amount of unlabeled data to match an improvement in generalization error with additional labeled data is a small constant, while for misspecified models, the asymptotic bias of the unlabeled-only approach grows with more dependencies.



%% Previous version
%Labeling data for modern machine learning is expensive and time-consuming. Weak supervision (WS) combines unlabeled data and weak sources to learn a model that generates a large labeled dataset. Such models can also benefit from labeled data, presenting a key tradeoff: should a user invest in few labeled or many unlabeled points? 
%We answer this via a framework centered on model misspecification in WS that is broadly applicable to method-of-moments estimators. Our core result is an exact bias-variance decomposition of the generalization error, which shows that the unlabeled-only approach incurs additional bias under misspecification. We then give a correction mechanism.
%We apply our framework to three scenarios---well-specified, misspecified, and corrected models---to 1) assess the value of labeled vs unlabeled data and 2) learn from a combination of both. In line with our error decomposition, we show theoretically that labeled data is more valuable for misspecified models and confirm this on synthetic data.
%On a real-world WS dataset, where our model is inevitably misspecified, using 40 labeled data points performs the same as using the entire unlabeled dataset of 1586 points, and combining the two types of data outperforms either alone.