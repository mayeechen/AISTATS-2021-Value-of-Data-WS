 \section{Introduction}

%What is the problem?
A key challenge in data-driven fields is the quality of training data. A fixed data collection budget can provide a large amount of noisy or incomplete training data, or a smaller but cleaner dataset. Given a choice between these two options, which should we select and which factors should determine this decision? This question, while fundamental in nature, is especially relevant to an important area of machine learning---semi- and weakly-supervised learning---where such datasets can be used to train an intermediate model that can output de-noised data to be used in a downstream procedure (e.g., training a deep model).

%This is particularly important in machine learning. The standard process in supervised learning has experts manually label individual examples, producing high-quality data. More recently, noisy, unlabeled data can be used to build weakly-super
%However, this is time-consuming and expensive. On the other hand, unlabeled data is easy to obtain but of lower quality.  As a result, there is a tradeoff between using a small amount of labeled data and a large amount of unlabeled data in constructing machine learning models. An important but understudied problem is how to select between two such datasets (and use them in conjunction if both are available) by understanding the relative value of data quality---here, as measured by type of supervision. 


% What is the problem?
% A key challenge in machine learning is collecting and curating datasets. Practitioners have instead turned to acquiring labels from noisier sources, including heuristic rules \citep{gupta2014improved}, crowdworkers \citep{karger2011iterative}, external knowledge bases \citep{hoffmann2011knowledge, mintz2009distant} and user-defined programs \citep{fu2020fast}. Frameworks such as distant supervision and weak supervision (WS), e.g., data programming \citep{Ratner16}, formalize the process of creating noisy labels from such user-defined sources. Such frameworks have become remarkably popular, with heavy use in industrial and academic settings.
%
% In WS processes, users provide \emph{labeling functions} operating on unlabeled data, the accuracies of these labeling functions are estimated, and these accuracy estimates are used to aggregate the source outputs into noisy labels. While WS frameworks allow for accuracy estimation \emph{directly from unlabeled data}, naturally, manual labels can also be used for this step. These two alternatives for non-manual dataset construction suggest the question of \emph{how resources should be allocated---into manually labeling examples, or gathering additional unlabeled data -- and how to combine both of them at once?} %and in what proportion?}


%Why is it interesting and important?
The scale of modern machine learning has pushed against the limits of budgets for building labeled datasets. To cope with this challenge, techniques that use few (or no) labeled points but have access to many unlabeled points along with distant or weak forms of supervision are used to learn an intermediate model for constructing larger labeled training datasets. Semi-supervised techniques \cite{zhu2009introduction} use a small number of labels to extend to other labeled datapoints \cite{Zhu02,Mann07,Wang08}, while the weak supervision framework, which we focus on, uses \bcw{Should we just say ``no'' here?} few (or no) labeled points and distant or weak supervision sources, such as heuristics~\cite{gupta2014improved,ratner2018snorkel},  external knowledge bases~\cite{mintz2009distant,craven:ismb99,takamatsu:acl12}, or noisy crowd-sourced labels~\cite{karger2011iterative,dawid1979maximum}.
%
These techniques benefit from adding more fully supervised signal and/or more unlabeled points. Therefore, a principle for choosing between the alternatives described above is important, yielding insights into important properties of data and enabling practitioners to choose how to invest a limited budget. We thus wish to develop a theoretical framework describing how to price labeled versus weakly labeled data. %Ideally, such a criterion quantifies an intuitive bias-variance tradeoff for source accuracy estimation---between fewer manually-written labels and far more numerous unlabeled points.

% Why is it hard? (E.g., why do naive approaches fail?)
To develop such an framework, we face several challenges. Classical statistical works, especially those analyzing model misspecification, a key ingredient for the WS setting, focus on asymptotic behavior of estimators \cite{kleijn2006, kleijn2012}. However, we are especially interested in the finite---and often very small---sample setting, in particular with respect to labeled data. On the other hand, empirical comparisons between supervised and unsupervised learning algorithms offer guidance on choosing datasets \cite{A,B,C}, but most of this work does not offer theoretical guarantees, or requires very restrictive assumptions. 

%Certainly, empirical comparisons between supervised and unsupervised learning algorithms can be used to develop decision thresholds for labeled versus unlabeled data based on sample size. However, most of these algorithms lack a theoretical framework where comparison between supervised and unsupervised settings can be made fairly. We therefore turn to the graphical model setup in weak supervision (WS), which can express both types of data and be used to produce predictors from both. In WS \citep{Ratner16}, practitioners can efficiently acquire weak labels from noisy sources, including heuristic rules \citep{gupta2014improved}, crowdworkers \citep{karger2011iterative}, external knowledge bases \citep{hoffmann2011knowledge, mintz2009distant} and user-defined programs \citep{fu2020fast}. These weak labels and assumed graphical model structure are 1) used to learn the model parameters, and 2) aggregated to produce a predictor for the latent response variable. This first parameter estimation step, however, can also be done with labeled data in conjunction with weak labels viewed as covariates. Therefore, motivated by weak supervision, we choose to analyze the performance of graphical models in the labeled and unlabeled data settings and use their generalization error to develop a criterion for valuing and combining data.

%However, graphical models are subject to model misspecification when the edge set describing the relationship among the weak sources and the true label is incorrect. These inaccuracies are propagated through the model and can result in estimated label distributions that are vastly different from the true label distribution. While many edge estimation procedures have been shown to approximately recover the dependency graph \citep{bach2017learning, varma2019learning}, they are often computationally challenging and do not guarantee correcting for misspecification. Furthermore, the asymptotic effects of misspecification have been explored for certain estimators [cite], but this does not include the efficient method-of-moments estimators commonly used in WS and also does not analyze the downstream impact of incorrect estimators on the predictors we construct. Finally, model misspecification has different consequences for the performance of algorithms learning from labeled versus unlabeled data, which is critical in establishing a value criterion.

%\bcw{In this paragraph, the alternatives are presented as ``labeled data'' and ``weakly labeled data'', but really it feels like the alternatives are ``labeled data'' and ``unlabeled data'', being used together to generate weak labels. I think that saying ``labeled data'' vs. ``weakly labeled data'' would make sense if these were each being fed into the end model, which isn't really our setup anymore (though is still part of the broader theme of the paper)}
We propose a theoretical model for the alternatives in weak supervision pipelines: fully supervised learning, or learning from weakly labeled data via a method-of-moments approach. Our error analysis for these two cases is centered on the principle of model misspecification in the form of unmodeled pairwise dependencies among weak sources of signal, and we show that it has important consequences for the value of labeled vs. weakly labeled data. We present a bias-variance decomposition of the generalization error, which in both cases consist of (i) irreducible error, (ii) variance, and (iii) bias due to model misspecification in statistical inference. We show that in the case of weakly labeled data, we incur an additional (iv) error term due to incorrectly estimating accuracies that scales with the extent of misspecification, $\mathcal{O}(d/m)$ for $m$ sources and $d$ unmodeled dependencies among them. %\steve{``true dependencies'' could be confusing, haven't connected dependencies with model misspecification yet}. 
This theoretical framework is then used in three applications. First, we discuss ways to correct for this misspecification via modifications to the method-of-moments algorithm for parameter estimation; in particular, a simple median approach can be beneficial for the case where $d = o(m^2)$ and is applicable to broader uses of method-of-moments algorithms that rely on conditional independence properties. Next, we develop a criterion for choosing between labeled and weakly labeled data, namely a minimum amount of labeled points needed to perform as well as a fixed amount of unlabeled points, depending on the amount and extent of misspecification. Lastly,
%and it is this additional bias that dominates the criterion we develop. 
we examine estimators that combine the biased parameters learned from the weakly labeled dataset with the unbiased ones learned from the labeled dataset. %propose using a James-Stein-like estimator to combine the biased parameters learned from the weakly labeled dataset with the unbiased ones from the labeled dataset. 

%What are the key components of my approach and results? Also include any specific limitations.
We verify the three consequences of our theoretical framework using experimental evidence on both real-world and synthetic datasets. First, we show that an algorithm that takes the median estimate of parameters is able to correct misspecification and results in a \textcolor{red}{X} improvement over using the mean estimate of parameters. Next, for real-world and synthetic datasets that exhibit model misspecification, we find that the threshold for the amount of weakly labeled data needed to result in equivalent performance as a fixed amount of labeled data is very high. This verifies the affect of the standing $\mathcal{O}(d/m)$ error offsetting the gap between the amount of weakly labeled versus labeled data. For example, on a spam classification task whose training set has $1586$ data points, we observe that $70$ gold labels provide a $3.2$ point increase in test set accuracy over learning exclusively from weakly labeled data. On an Amazon reviews classification task with $n = 12600$ data points, we observe that $30$ gold labels provide a $6.2$ point increase in test accuracy. On the other hand, on synthetic data with no model misspecification the value of labeled vs. weakly labeled data is limited to roughly a ratio of $5$ weakly labeled points per $1$ fully labeled point. 
%For example, for reasonable data distributions and labeling functions, the value ratio is less than $5$, meaning that the performance gain from acquiring $1$ gold labeled point is less than the gain from acquiring $5$ more weakly labeled points.
% On the other hand, for real-world and synthetic datasets that indeed have dependencies between labeling functions, the threshold for weakly labeled data to yield performance gains is much higher, verifying the affect of the standing $\mathcal{O}(d/m)$ error. 
We also show that the standing bias resulting from misspecification persists when training popular downstream models on our generated datasets as done in standard weak supervision pipelines. Lastly, we verify that %our algorithm for reducing misspecification results in a \textcolor{red}{X} improvement, and 
the combined estimator we propose offers a \textcolor{red}{X} improvement over using just the weakly labeled data, and a \textcolor{red}{X} improvement over using just the labeled data. %Furthermore, our criterion and combination tool can be automatically integrated into modern ML pipelines, maximizing model performance for any budget.


%Existing results in the weak supervision literature establish that, under certain assumptions, the accuracy estimators used in weak supervision are consistent---but such assumptions are unlikely to be met in practice due to dependencies among the weak sources of labels. Experience with WS frameworks in industry has revealed that there is a ratio---a threshold factor for how much larger the weakly-supervised dataset must be to ensure superior performance downstream compared to one built using a labeled dataset. These empirical observations, however, are based on rules-of-thumb and received knowledge---and vary from task to task. We seek a principled approach for 1) choosing between a labeled and unlabeled dataset of different sizes using a \textit{data value criterion}, and 2) selecting a \textit{combined estimator} applying both types of data, and that accounts for the bias-variance tradeoffs.


% Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
%We center our investigation on the principle of model misspecification. We show that this has important consequences for the value of labeled vs. unlabeled data---but has not been explored in previous works on weak supervision. We propose a theoretical model for the alternatives in weak supervision pipelines. 
%While simple, our model characterizes all of the complexity in distinguishing between these alternatives. 
%Concretely, we establish lower and upper bounds on generated label error. 




% One of the key challenges of practical machine learning is collecting and curating large labeled datasets. The standard process of experts manually labeling individual examples is expensive and time consuming, often prohibitivley so. Recently, practitioners have instead gathered labels from noisier sources, including heuristic rules \citep{gupta2014improved}, crowdworkers \citep{karger2011iterative}, external knowledge bases \citep{hoffmann2011knowledge, mintz2009distant} and user-defined programs \citep{alex2016data, ratner2019training, fu2020fast}. Data programming was introduced by \cite{alex2016data} to formalize the process of creating noisy labels from user-defined programs, referred to as \textit{labeling functions}, which make a prediction or abstain for each example. For example, for a sentiment classification task, a reasonable labeling function might predict positive sentiment if the word ``good'' is present and abstain otherwise. In the data programming framework, the user provides several labeling functions, the accuracies of these labeling functions are estimated and these accuracy estimates are used to aggregate their outputs into noisy labels.

% Data programming suggests a fundamentally different approach for using expert time: instead of labeling data points individually, experts write labeling functions incorporating high-level intuitions. This alternative approach raises the question of how expert time should be spent most effectively. We take a first step towards answering this question by exploring the value of a small number of labeled data points within the data programming pipeline. In other words, we explore when expert time would best be spent on \textit{both} labeling functions and labeling a small number of examples individually.

% We propose a simple method for effectively using small amounts of labeled data on top of labeling functions to produce noisy labels, based on the maximum likelihood estimation approach introduced by \cite{alex2016data} and common semi-supervised techniques \citep{zhu2009introduction}. Specifically, we define separate log-likelihood objectives over the unlabeled and labeled data, where we marginalize over the unobserved gold labels for the unlabeled data, and maximize a weighted average of the two. Like previous approaches, our model assumes that labeling functions are conditionally independent given the gold label. We investigate the value of labeled data in two settings: where this assumption is met, and where (more realistically) some dependencies exist between among functions.
% % Modern machine learning algorithms require large amounts of labeled data, but collecting gold labels for individual data points is expensive and time consuming, often prohibitively so. Noisy labels from sources including crowdworkers \citep{karger2011iterative}, heuristic rules \citep{gupta2014improved}, user-defined programs \citep{alex2016data, fu2020fast} and external knowledge bases \citep{hoffmann2011knowledge, mintz2009distant} are increasingly being used as substitutes where acquiring gold labels is impractical. Data programming was introduced by \cite{alex2016data} as a paradigm for creating noisy training labels from user-defined programs known as \textit{labeling functions}. For example, for a sentiment classification task the user might define a labeling function which predicts positive sentiment if the word ``good'' is present. The outputs of multiple labeling functions are aggregated, typically by estimating the accuracy of each labeling function and weighting its predictions according to its accuracy.
% % Data programming posits that the most efficient use of human expert time is writing labeling functions, rather than labeling individual data points. The data programming pipeline does make use of small amounts of labeled data to guide the development of labeling functions, but estimates labeling function accuracies using \textit{only unlabeled data}. This requires making assumptions about the behavior of labeling functions: standard data programming approaches assume that labeling functions (or subsets of labeling functions) are conditionally independent given the gold label. When these assumptions are met, data programming works remarkably well, with estimates of labeling function accuracies converging to the true accuracies as the available unlabeled data grows. However, for many real datasets dependencies between labeling functions mean that accuracies cannot be estimated perfectly. With these two settings in mind, we explore whether small amounts of labeled data can be valuable to better estimate the accuracies of labeling functions and thus improve the quality of the noisy labels produced.
% % We propose a simple method for effectively using small amounts of labeled data in the data programming pipeline, based on the maximum likelihood estimation approach introduced by \cite{alex2016data} and common semi-supervised techniques \citep{zhu2009introduction}. Specifically, we define separate log-likelihood objectives over the unlabeled and labeled data, where we marginalize over the unobserved gold labels for the unlabeled data, and maximize a weighted average of the two. Unlike previous approaches, our method models different probabilities of labeling functions outputting an answer and outputting the correct answer for different gold labels, which we find improves empirical performance.

% In the setting where labeling functions are conditionally independent given the gold label, we find that the value of labeled data is limited. We define the \textit{labeled data value ratio} to be the ratio of the number of additional labeled to unlabeled data points required to attain some target accuracy. We observe empirically that
% \begin{itemize}
%     \item The labeled data value ratio \textit{does not} depend on the target accuracy or the number of currently available data points. In other words, it is a property of the data distribution and labeling functions.
%     % \item The labeled data value ratio decreases as the accuracies of labeling functions, coverages of labeling functions, and the number of labeling functions increase.
    
%     \item For reasonable data distributions and labeling functions, the labeled data value ratio is less than $5$, meaning that the performance gain from acquiring $1$ gold labeled point is less than the gain from acquiring $5$ more unlabeled points.
% \end{itemize}

% When dependencies exist among labeling functions, we find that under the same conditional independence assumptions learning from labeled data produces significantly better noisy labels than learning from unlabeled data. That is, labeled data provides robustness to the mis-specified model. We define a synthetic dataset with reasonable (but not conditionally independent) labeling functions, for which we show that
% \begin{itemize}
%     \item Using the same conditionally independent model, learning from labeled data asymptotically produces significantly better noisy labels learning than unlabeled data.
    
%     \item Only a small number of labeled data points (less than $100$ where the unlabeled datasets size is $10,000$) are needed to realize significant performance gains from labeled data.
% \end{itemize}

% We finally demonstrate results on realistic datasets. Naturally, limited dependencies exist between labeling functions for these real datasets, meaning that labeled data provides performance gains even with large amounts of available unlabeled data. On the Spam dataset whose training set has $1586$ data points, we observe that $70$ gold labels provide a $3.2$ point increase in test set accuracy over learning exclusively from unlabeled data. On the Amazon Reviews dataset whose training set has $12600$ data points, we observe that $30$ gold labels provide a $6.2$ point increase in test set accuracy.

