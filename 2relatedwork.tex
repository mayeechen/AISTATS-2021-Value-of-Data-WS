 \section{Related Work}
%\paragraph{Method-of-Moments Estimators}
%A tranche of latent variable model literature relies on the method-of-moments approach. These estimation approaches 
%often involve decomposing multiple observable ``views'' of latent variables. The decomposition is applied to closed-form systems of equations~\citep{joglekar2013evaluating, fu2020fast} and tensors~\citep{anandkumar2014tensor, chaganty2014estimating, Bhaskara14}, but in all cases, conditional independence among the views is required for the structural relationships to hold. We focus on a particular approach in this paper, but our analysis can provide error decomposition under misspecification for other method-of-moments estimators.

\paragraph{Misspecification in Graphical Models}
The asymptotic effect of misspecification on parameter estimation is studied by \cite{kleijn2012}, extending the Bernstein-Von Mises theorem to cases where observed samples are not of the parametric distribution being estimated. %They find that certain estimators, such as MLE, converge to a normal distribution centered at the point that results in a parametrized distribution closest in Kullback-Leibler divergence to the true distribution, and this property is directly used in asymptotic analysis of estimators in Bayesian and variational inference \cite{wang2019variational, hongmartin2020}. 
However, their main results do not fully extend to method-of-moments estimators. Other analyses of model misspecification directly examine families of models, such as \cite{JogL15}'s lower bound on KL-separation of Gaussian graphical models. While this bound is important for modeling errors in inference,
%a version of their mutual information bounds appear as an inference bias in our error analysis, 
it does not illustrate our additional error in parameter estimation. More generally, works on misspecification either study a particular class of techniques~\citep{Blasi13} or a particular model and propose repairs~\citep{Grunwald17}---while our goal is to compare its effects on alternative datasets.

\paragraph{Structure Learning}
One way to reduce misspecification is to produce a more refined model. Graphical model structure learning aims to do so in both the supervised~\citep{Ravikumar11, Loh13} and unsupervised cases~\citep{Chandrasekaran12,Meng14, bach2017learning, varma2019learning}. However, these works present computational challenges, require (often strong) conditions to hold, and do not analyze the downstream impact of errors. Our approach instead focuses on understanding the impact of such errors and is also applicable to partial recovery that often results from these structure learning techniques.

%\paragraph{Semi-Supervised Learning} involves using a small set of labeled points and a larger set of unlabeled points~\citep{Chapelle09, zhu2009introduction}. There are a wide variety of such techniques: the missing labels can be modeled as latent variables and expectation maximization (EM) can be used~\citep{Nigam2000}. Another approach uses a graph structure to propagate labels, relying on MRFs~\citep{Zhu02} or kernels~\citep{Kondor02}. In contrast to such techniques, WS can operate with no labels. It does not focus on extending any given labels, but rather learning the weak sources that can be applied to any data. 

%does not require any labels, and by learning the accuracies of sources that can then be applied to any unlabeled data


%\todo{maybe data shapley or stuff like that?}
%There has also been recent work in data valuation \citep{ghorbani2019data}, but they focus on a per-point valuation for a fixed supervision setting rather than a comparison between labeled and unlabeled data approaches. 