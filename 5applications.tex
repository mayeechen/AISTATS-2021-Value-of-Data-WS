\section{Applications}

Based on our generalization error framework, we now have a rigorous way to analyze WS settings with misspecification. We examine the three following practical applications of our theoretical results:
\begin{itemize}
    \item \textbf{Correcting for misspecification:} for the weakly labeled data setting, we propose a simple algorithm improving over random selection of triplets $\lf_i, \lf_j, \lf_k$ that removes the error term $\varepsilon_{\max} \left(\frac{c_1 d}{m} + \frac{c_2 m}{\sqrt{n_U}} + \frac{c_3 m}{n_U}\right)$ given that $m$ is large and $d$ is $o(m^2)$.
    \item \textbf{Choosing between labeled and unlabeled datasets:} we address our motivating question about the value of labeled data - is a few labeled samples or many weakly labeled samples better? This decision depends on misspecification ($d$, $\varepsilon_{\max}$) and $n_U$ versus $n_L$ based on Theorems \ref{thm:labeled} and \ref{thm:unlabeled}.
    \item \textbf{Combining labeled and unlabeled data:} we explore linear combinations of the estimators, as well as a James-Stein type estimator proposed by \cite{GreenStrawderman1991} that combines an unbiased and biased estimator. %Based on our misspecification analysis, we show that this estimator offers a \textcolor{red}{X} improvement in generalization error. 
\end{itemize}

In this section, we discuss these three consequences of our framework theoretically. In section \ref{sec:exp}, we verify them on synthetic and real data.

\subsection{Mitigating Misspecification}
How can we reduce the penalty for dealing with dependencies? This is a common problem in method-of-moments approaches to latent variable estimation beyond weak supervision \cite{anandkumar12, chaganty14} --- in particular, multi-view learning --- where most literature assumes the dependencies are known or that the true data distribution exhibits conditional independence among all observable variables. We examine how to reduce misspecification for our particular method described in \eqref{eq:triplet}, but we also discuss how our correction can be applied to other algorithms in the Appendix to show its use extended to other weak supervision approaches and more general latent variable estimation problems.

%Note that this correction for misspecification is applicable to many method-of-moments approaches beyond its use in WS, such as \cite{anandkumar12, chaganty14}, in order to more generally learn parameters of latent variable models. 

In our parameter estimation approach, if there exists an $\lf_i$ such that there are no $\lf_i, \lf_k$ where all three labeling functions are pairwise conditionally independent given $Y$, then it is not possible to learn  $a_i$. In less demanding cases, %the natural approach is to learn these dependencies via \emph{structure learning}. This can be done in the unlabeled data setting \citep{bach2017learning, varma2019learning}. However, it may involve solving a challenging optimization problem (e.g., an SDP). 
we suggest an alternative approach based on \emph{medians}. Note that the contribution to the accuracy estimation error coming from misspecification is due to including triplets that violate pairwise conditional independence
%,  of which there are $O(d/m^2)$, 
in our average $\widetilde{a}_i^U$. To reduce this impact, rather than computing the average over all triplets, we estimate each $a_i$ by computing the median accuracy over all pairs $\lf_j, \lf_k$ using \eqref{eq:triplet} a total of ${m - 1 \choose 2}$ times.
\begin{proposition}
%Denote $\widetilde{a}_i^{(j, k)}$ as the estimate in \eqref{eq:triplet}. 
When $\widetilde{a}_i^U = \mathrm{median}(\{\widetilde{a}_i^{(j, k)} \; \forall \; j, k \neq i \})$, then $\widetilde{a}_i^U$ is not affected by misspecification and is thus a consistent estimator if $m > 5$ and $d \le \frac{(m - 1)(m - 2)}{4}$. \todo{sampling noise...}
\end{proposition}

Note that while we assume sparsity of $E_{\lf}$ for simpler analysis in section \ref{sec:theory}, the conditions above on $d$ and $m$ apply for any given dependency graph in the WS setting. If these conditions hold, the upper bound on excess generalization error after using medians of accuracies is $\frac{c_4 m}{n_U} + d \log 2$, and we verify this improvement in generalization error in section \ref{sec:exp}. 

%Finally, this median-based approach applies to other steps of the label model. A similar technique, where we partition the sources into $q$ groups, perform inference separately with each group to get $\widetilde{Y}$, and take the median, can be used to reduce the impact of misspecification in the approximate inference step.

%This reduces the probability of selecting an unbiased triplet (as the median) to $O(1-(d/m^2)^{t+1})$. This technique helps us in cases where $d = o(m^2)$. 


\subsection{Understanding the value of labeled data}
We use our analysis from section \ref{subsec:scaling} to develop a criterion for deciding between $n_L$ labeled points and $n_U$ weakly labeled points. Let $n = n_U$, and then compute
\[\alpha(n) = \min_{n_L \in \mathbb{N}} \text{ s. t. } U^{\text{unlabeled}}(n) \leq U^{\text{labeled}}(n_L),\]
where $U^{\text{unlabeled}}, U^{\text{labeled}}$ are the upper bounds on the excess generalization error for unlabeled and labeled data, respectively.
Then, the ratio
%\begin{align}
$V(n) = {n}/\alpha(n)$
%\end{align}
represents the value of labeled over weakly labeled data. The intuitive idea here is to compare, for each amount of weakly labeled data $n_U$, what factor less labeled data we would require to produce an equivalent error bound. 
\todo{Rewrite the rest of this section with $\alpha$ defined as a function of weakly labeled points, rather than labeled points.} Based on Theorems \ref{thm:labeled} and \ref{thm:unlabeled}, we see that this ratio captures the tradeoff between $\varepsilon_{\max} \big(\frac{c_1 d}{m} + \frac{c_2 m}{\sqrt{n_U}} + \frac{c_3 m}{n_U} \big) + \frac{c_4 m}{n_U}$ and $\frac{m}{2n_L}$. In particular, %for a weakly labeled dataset to have a better generalization error, 
we must have that $\alpha(n) = c\big(\frac{\varepsilon_{\max} n m^2}{m^2 - \varepsilon_{\max} d n}\big)^2$ for some constant $c$ and $n < \frac{m^2}{\varepsilon_{\max} d}$; when $n$ is too large, it is possible that no amount of unlabeled data can achieve the same generalization error due to the standing bias. Therefore, as any  one of $n, \varepsilon_{\max}$, or $d$ increases, the threshold for weakly labeled points becomes higher. On the other hand, if we were to ignore misspecification, we only require that $V(n) = 2c_4$ be a constant value, which as a criterion could lead to wrong decisions if the model is actually incorrect.
% We use our analysis from section \ref{subsec:scaling} to develop a criterion for deciding between $n_L$ labeled points and $n_U$ weakly labeled points. Let $n = n_L$, and then compute
% \[\alpha(n) = \min_{n_U \in \mathbb{N}} \text{ s. t. } U^{\text{unlabeled}}(n_U) \leq U^{\text{labeled}}(n),\]
% where $U^{\text{unlabeled}}, U^{\text{labeled}}$ are the upper bounds on the excess generalization error for unlabeled and labeled data, respectively.
%
% Then, the ratio
% %\begin{align}
% $V(n) = \alpha(n)/{n}$
% %\end{align}
% represents the value of labeled over weakly labeled data. The intuitive idea here is to compare, for each amount of labeled data $n$, what factor greater unlabeled data we would require to produce an equivalent error bound. 
% Based on Theorems \ref{thm:labeled} and \ref{thm:unlabeled}, we see that this ratio captures the tradeoff between $\varepsilon_{\max} \big(\frac{c_1 d}{m} + \frac{c_2 m}{\sqrt{n_U}} + \frac{c_3 m}{n_U} \big) + \frac{c_4 m}{n_U}$ and $\frac{m}{2n_L}$. In particular, %for a weakly labeled dataset to have a better generalization error, 
% we must have that $\alpha(n) = c\big(\frac{\varepsilon_{\max} n m^2}{m^2 - \varepsilon_{\max} d n}\big)^2$ for some constant $c$ and $n < \frac{m^2}{\varepsilon_{\max} d}$; when $n$ is too large, it is possible that no amount of unlabeled data can achieve the same generalization error due to the standing bias. Therefore, as any  one of $n, \varepsilon_{\max}$, or $d$ increases, the threshold for weakly labeled points becomes higher. On the other hand, if we were to ignore misspecification, we only require that $V(n) = 2c_4$ be a constant value, which as a criterion could lead to wrong decisions if the model is actually incorrect.

%Equivalently, we require that $d$ must be of order  $o\big(\frac{m^2}{\varepsilon_{\max}}\big(\frac{1}{n_L} - \frac{1}{\sqrt{n_U}}\big)\big)$ for a weakly labeled dataset to have better generalization error. 


%Therefore, when $d$ and $\varepsilon_{\max}$ are both small while $n_L$ is also small, $V(n)$ should suggest a reasonably-sized $n_U$ in order for an unlabeled dataset to yield higher performance. On the other hand, when there are many unmodeled dependencies and all of them are strong, $V(n)$ should be very large, suggesting that a small labeled dataset performs better.

\subsection{Combining labeled and unlabeled data}
While we now have a criterion to choose between datasets, how do we combine information from both? We examine ways to combine the accuracy parameters from the labeled and weakly labeled datasets,  $\widetilde{a}^L = \Ehat{\bm{\lf} Y}$ and $\widetilde{a}^U$ as defined in \eqref{eq:triplet} respectively. Recall that $\widetilde{a}^L$ is unbiased, while $\widetilde{a}^U$ is both biased and inconsistent. 

First, we consider a simple linear combination, $a^{\mathrm{lin}} = \alpha \widetilde{a}^U + (1 - \alpha) \widetilde{a}^L$ for some weight $\alpha \in [0, 1]$. We produce an upper bound on excess generalization error from using $a^{\mathrm{lin}}$ similar to our results in Theorems \ref{thm:labeled} and \ref{thm:unlabeled}.

\begin{corollary}
Define $R_{\mathrm{lin}}^{\mathrm{excess}}(\alpha)$ as the excess generalization error from using $a^{\mathrm{lin}}$ with weight $\alpha$. Then,
\begin{align}
    R_{\mathrm{lin}}^{\mathrm{excess}}(\alpha)\le & \varepsilon_{\max}\left(\frac{c_1 \alpha d }{m} + \frac{c_2 \alpha^2 m}{\sqrt{n_U}} + \frac{c_3 \alpha^2 m}{n_U} \right) \nonumber \\
    +& \frac{c_4 \alpha^2 m}{n_U} + \frac{(1 - \alpha)^2 m}{n_L} + d\log 2 + o(1/n_U) \nonumber
\end{align}
\end{corollary}

Depending on the values of $\varepsilon_{\max}, d, m, n_U, n_L$ and the constants, it is possible that this upper bound is minimized using some $\alpha \in (0, 1)$, which would suggest that a simple linear combination of the two estimators does better than using either one of them. \todo{double check this corollary, more analysis}. We test a range of $\alpha$ on our synthetic datasets to confirm this.

%From Lemma \ref{lemma:accuracy_bias}, we see that $\widetilde{a}_i^U$ is biased while $\widetilde{a}_i^L$ is not. At first glance, it would seem that we should simply rely on the unbiased estimator---however, as known from Stein's phenomenon, this is not necessarily the right approach.
While a linear combination can yield better performance under certain circumstances, there have been more sophisticated James-Stein-type estimators proposed that take bias into account and have been shown to dominate the MLE estimator. In particular, we draw inspiration from \cite{GreenStrawderman1991, GreenStrawderman2001}, who propose the following combined estimator:
\begin{align}
    \bar{a} := \widetilde{a}^U + \bigg(1 - \frac{r}{(\widetilde{a}^L - \widetilde{a}^U)^T \Sigma^{-1} (\widetilde{a}^L - \widetilde{a}^U)} \bigg)_{\mathclap{+}} (\widetilde{a}^L - \widetilde{a}^U),
\end{align}
%\begin{align}
%    \bar{a}_i := \widetilde{a}_i^L - \frac{(m - 2) }{n_L \|\widetilde{a}_i^L - \widetilde{a}_i^U \|_2^2} (\widetilde{a}_i^L - \widetilde{a}_i^U)
%\end{align}
where $\Sigma = \Cov(\widetilde{a}^L)$, and $r \in [0, 2(m - 2)]$.  They show that this estimator dominates $\widetilde{a}^L$ when the unbiased estimator is Gaussian and its covariance is known. However, since we can only estimate the covariance matrix, we replace $\Sigma$ with an empirical estimate $\hat{\Sigma}$. Moreover, since $\widetilde{a}^L$ is only Gaussian asymptotically, we do not provide theoretical guarantees on $\bar{a}$ but instead empirically validate its performance.

%This estimator has been proven to dominate the unbiased $\widetilde{a}^L$ when the true covariance matrix $\Sigma$ is known, even when $\widetilde{a}^U$ is very biased \steve{we don't have normality though, right?}. %In particular, \cite{GreenStrawderman1991} show that
%Using their equation 2.5, it is true that
%\begin{align}
%    &\E{}{\|\bar{a} - a \|_2^2} \le m Var(\widetilde{a}_i^L) - \nonumber \\
%    &\frac{Var(\widetilde{a}_i^L)^2 (m - 2)^2}{\|\E{}{\widetilde{a}^U} - a \|_2^2 + (m - 2)(\sqrt{Var(\widetilde{a}_i^L)} + Var(\widetilde{a}_i^U))} \nonumber
%\end{align}

%which, when plugged into our decomposition framework in Theorem \ref{thm:decomposition}, gives us a better generalization error \todo{be more precise here.}

%We now show that the label model generalization error using $\bar{a}$ is better than that using either $\widetilde{a}_i^U$ or $\widetilde{a}_i^L$. \todo{plug above eqn into thm 1 type bound}.
